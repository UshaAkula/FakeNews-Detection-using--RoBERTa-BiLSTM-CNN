# -*- coding: utf-8 -*-
"""RBL_FakeNewsDet

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11-OSPhuC86hkjktdlWj-obRjEl7n8-Hp
"""

!pip install -q kaggle

from google.colab import files
files.upload()  # upload kaggle.json

!ls

import pandas as pd
from transformers import RobertaTokenizer
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader

df=pd.read_csv('WELFake_Dataset.csv')
df=df[['text','label']].dropna().drop_duplicates()
df['text']=df['text'].str.lower().str.replace(r'http\S+|[^a-z\s]',' ',regex=True).str.replace(r'\s+',' ',regex=True)

tok=RobertaTokenizer.from_pretrained('roberta-base')

class FakeNewsDataset(Dataset):
    def __init__(self,texts,labels,tokenizer,max_len=256):
        self.texts=texts
        self.labels=labels
        self.tokenizer=tokenizer
        self.max_len=max_len
    def __len__(self):return len(self.texts)
    def __getitem__(self,idx):
        t=self.texts[idx]
        label=torch.tensor(self.labels[idx],dtype=torch.float)
        enc=self.tokenizer(t,padding='max_length',truncation=True,max_length=self.max_len,return_tensors='pt')
        return {'input_ids':enc['input_ids'].squeeze(),
                'attention_mask':enc['attention_mask'].squeeze(),
                'label':label}

from sklearn.model_selection import train_test_split

# First split: 90% train+val, 10% test
X_temp, X_test, y_temp, y_test = train_test_split(
    df['text'].tolist(), df['label'].tolist(),
    test_size=0.1,
    stratify=df['label'],
    random_state=42
)

# Second split: From 90% temp â†’ 81% train, 9% val
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp,
    test_size=0.1,  # 10% of 90% = 9%
    stratify=y_temp,
    random_state=42
)
train_ds = FakeNewsDataset(X_train, y_train, tok)
val_ds   = FakeNewsDataset(X_val, y_val, tok)
test_ds  = FakeNewsDataset(X_test, y_test, tok)

train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)
val_dl   = DataLoader(val_ds, batch_size=16)
test_dl  = DataLoader(test_ds, batch_size=16)

import pandas as pd
df = pd.read_csv('WELFake_Dataset.csv')
print(df.head())
print(df.info())
print(df['label'].value_counts())
print(f"Total samples: {len(df)}")

import torch
import torch.nn as nn
from transformers import RobertaModel

class RobertaBiLSTM_CNN(nn.Module):
    def __init__(self, hidden_size=128, lstm_layers=1, dropout=0.3, cnn_out=64, freeze_roberta=True):
        super(RobertaBiLSTM_CNN, self).__init__()
        self.roberta = RobertaModel.from_pretrained('roberta-base')

        # Freeze all layers
        if freeze_roberta:
            for param in self.roberta.parameters():
                param.requires_grad = False
            # Unfreeze last 2 encoder layers
            for param in self.roberta.encoder.layer[-2:].parameters():
                param.requires_grad = True

        self.lstm = nn.LSTM(
            input_size=self.roberta.config.hidden_size,
            hidden_size=hidden_size,
            num_layers=lstm_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout
        )

        self.conv1 = nn.Conv1d(
            in_channels=hidden_size * 2,
            out_channels=cnn_out,
            kernel_size=3,
            padding=1
        )
        self.relu = nn.ReLU()
        self.pool = nn.AdaptiveMaxPool1d(1)

        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(cnn_out, 1),
            nn.Sigmoid()
        )

    def forward(self, input_ids, attention_mask):
        out = self.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        lstm_out, _ = self.lstm(out)                      # (B, T, H*2)
        lstm_out = lstm_out.permute(0, 2, 1)              # (B, H*2, T)
        cnn_out = self.pool(self.relu(self.conv1(lstm_out))).squeeze(-1)  # (B, cnn_out)
        return self.classifier(cnn_out).squeeze()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model2 = RobertaBiLSTM_CNN().to(device)

from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR

optimizer2 = AdamW(model2.parameters(), lr=2e-5)
loss_fn2 = nn.BCELoss()
scheduler2 = CosineAnnealingLR(optimizer2, T_max=3)

import copy

best_val_acc = 0
wait = 0
patience = 2
best_model_state = copy.deepcopy(model2.state_dict())

from tqdm import tqdm

def train_model(model, train_dl, val_dl, epochs=5):
    global best_val_acc, wait, best_model_state
    for epoch in range(epochs):
        model.train()
        total_loss, correct, total = 0, 0, 0

        for batch in tqdm(train_dl, desc=f"Epoch {epoch+1} Training"):
            input_ids = batch['input_ids'].to(device)
            mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            optimizer2.zero_grad()
            preds = model(input_ids, mask)
            loss = loss_fn2(preds, labels)
            loss.backward()
            optimizer2.step()

            preds_cls = (preds > 0.5).float()
            correct += (preds_cls == labels).sum().item()
            total_loss += loss.item()
            total += len(labels)

        scheduler2.step()
        train_acc = correct / total
        val_loss, val_acc = evaluate_model(model, val_dl)

        print(f"\nEpoch {epoch+1} Summary:")
        print(f"Train Accuracy    : {train_acc:.4f}")
        print(f"Validation Accuracy: {val_acc:.4f}")
        print(f"Validation Loss    : {val_loss:.4f}")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_model_state = copy.deepcopy(model.state_dict())
            wait = 0
            print("" Best model saved.\n")
        else:
            wait += 1
            print("No improvement.")
            if wait >= patience:
                print("Early stopping triggered.")
                break

    model.load_state_dict(best_model_state)
    return model

from sklearn.metrics import accuracy_score, classification_report

def evaluate_model(model, dataloader):
    model.eval()
    correct, total, loss_total = 0, 0, 0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            preds = model(input_ids, mask)
            loss = loss_fn2(preds, labels)

            preds_cls = (preds > 0.5).float()
            correct += (preds_cls == labels).sum().item()
            total += len(labels)
            loss_total += loss.item()
            all_preds.extend(preds_cls.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    acc = correct / total
    return loss_total / len(dataloader), acc

trained_model2 = train_model(model2, train_dl, val_dl, epochs=3)

trained_model2.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for d in test_dl:
        input_ids = d['input_ids'].to(device)
        mask = d['attention_mask'].to(device)
        labels = d['label'].to(device)
        preds = trained_model2(input_ids, mask)
        preds_cls = (preds > 0.5).float()
        all_preds.extend(preds_cls.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

print(" Final Test Accuracy:", accuracy_score(all_labels, all_preds))
print(classification_report(all_labels, all_preds, digits=4))

